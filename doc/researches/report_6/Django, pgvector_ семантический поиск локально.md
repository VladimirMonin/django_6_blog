# **Архитектура систем семантического поиска на базе Django и PostgreSQL: Глубокий анализ инструментов и методов (Декабрь 2025\)**

## **1\. Введение: Смена парадигмы в поиске и RAG-системах**

К концу 2025 года индустрия разработки интеллектуальных поисковых систем и приложений Retrieval-Augmented Generation (RAG) переживает фундаментальную трансформацию. Период экспериментов с разрозненными инструментами и специализированными векторными базами данных сменяется этапом консолидации и архитектурного упрощения. Если еще два года назад стандартом считалось использование отдельного сервиса для хранения векторов (такого как Pinecone или Weaviate), то сегодня наблюдается массовый возврат к монолитным хранилищам данных, где реляционные и векторные данные сосуществуют в едином пространстве. Центральное место в этой экосистеме занял **PostgreSQL** с расширением **pgvector**, которое к версии 0.8.0 достигло функционального паритета со специализированными решениями, предлагая при этом неоспоримое преимущество транзакционной целостности и упрощения инфраструктуры.  
Для разработчиков, использующих стек **Django**, это открывает возможности построения высокопроизводительных поисковых систем без необходимости внедрения новых микросервисов. Однако этот подход переносит сложность с уровня инфраструктуры на уровень логики приложения и структуры данных. Ключевым вызовом становится не "где хранить", а "как подготовить" данные.  
Данный отчет представляет собой исчерпывающее руководство по построению системы семантического поиска, отвечающей требованиям декабря 2025 года: локальная обработка и чанкинг текста, векторизация через внешние API (например, OpenAI), хранение в PostgreSQL и реализация гибридного поиска с использованием Django ORM. Мы подробно рассмотрим эволюцию инструментов локального чанкинга, таких как **semchunk**, **chonkie** и **ai-chunking**, проанализируем математические основы гибридного ранжирования (RRF) и предоставим архитектурные паттерны для их реализации.

### **1.1. Контекст задачи: Почему локальный чанкинг критичен?**

В архитектуре, где векторизация делегируется внешнему API (например, моделям серии text-embedding-3), роль локальной предобработки текста (чанкинга) становится критической по двум причинам: экономика и качество.  
Во-первых, API провайдеры тарифицируют услуги по количеству входных токенов. Отправка "сырого" текста или плохо нарезанных фрагментов с избыточным перекрытием (overlap) ведет к прямой финансовой неэффективности. Во-вторых, и это более важно, качество семантического поиска напрямую зависит от семантической целостности фрагмента. Феномен, известный как "Lost in the Middle" (потеря контекста в середине), усугубляется, если границы чанка разрывают логическую цепочку рассуждений или смешивают несколько разнородных тем в один вектор.  
Локальный чанкинг позволяет разработчику полностью контролировать структуру подаваемых на вход модели данных, используя ресурсы собственного сервера (CPU/RAM) для интеллектуального разбиения, прежде чем платить за вычисление эмбеддинга. В 2025 году появились инструменты, позволяющие делать это не просто по счетчику символов, но с учетом семантики и структуры документа, не покидая Python-окружения вашего Django-проекта.

## **2\. Эволюция и анатомия инструментов локального чанкинга**

Процесс разбиения текста на фрагменты (чанки) прошел путь от простых регулярных выражений до сложных алгоритмов, использующих локальные нейросети. В экосистеме Python к концу 2025 года выделились три ключевых инструмента, каждый из которых реализует свою философию обработки текста: **Semchunk**, **Chonkie** и **AI-Chunking**.

### **2.1. Semchunk: Философия скорости и детерминизма**

Библиотека **semchunk**, достигшая к октябрю 2025 года версии 3.2.5, представляет собой вершину эволюции "быстрых" методов чанкинга. Её создатель, Isaacus, сфокусировался на проблеме производительности и точности подсчета токенов.

#### **Алгоритмический подход**

В основе semchunk лежит не нейросетевая модель, а сложный рекурсивный алгоритм, работающий в тандеме с токенизатором целевой модели (например, tiktoken для моделей OpenAI). Это критически важное отличие от наивных сплиттеров, считающих символы. semchunk гарантирует, что созданный чанк никогда не превысит лимит токенов модели, так как он считает именно токены, а не символы, в процессе разбиения.  
Алгоритм работает по принципу "сверху-вниз" с последующим слиянием :

1. **Семантическая иерархия разделителей:** Инструмент сначала пытается разбить текст по "сильным" границам (параграфы, двойные переносы строк).  
2. **Рекурсивное уточнение:** Если полученный фрагмент превышает заданный chunk\_size (например, 512 токенов), он рекурсивно разбивается по менее значимым разделителям (предложения, части предложений, слова).  
3. **Интеллектуальное слияние (Re-merging):** После дробления алгоритм проходит "снизу-вверх", склеивая соседние мелкие фрагменты обратно, пока они укладываются в лимит. Это предотвращает появление "осколков" текста (например, одиночных слов), которые часто возникают при использовании простых рекурсивных сплиттеров из старых библиотек вроде LangChain.

#### **Преимущества в контексте Django**

Для Django-приложения semchunk идеален своей легковесностью. Он не требует установки тяжелых библиотек вроде torch или transformers, если вы используете tiktoken. Это означает, что Docker-образ вашего проекта останется компактным. Скорость работы semchunk позволяет обрабатывать гигабайты текста в реальном времени, не блокируя воркеры Celery или потоки сервера.

### **2.2. Chonkie: Семантическая точность и "Поздний Чанкинг"**

Библиотека **chonkie** (талисман — карликовый бегемот) — это продукт 2025 года, созданный как ответ на потребность в более "умном", но простом в использовании чанкинге. Если semchunk — это инструмент для быстрой токенизации, то chonkie фокусируется на смысловой нагрузке.

#### **Семантический чанкинг (SemanticChunker)**

chonkie реализует подход, при котором границы чанка определяются не синтаксисом (точками или абзацами), а смыслом. Библиотека использует легковесные локальные модели эмбеддингов (через интеграцию с model2vec или квантованные модели sentence-transformers), чтобы вычислять векторы для каждого предложения.  
Процесс выглядит так:

1. Текст разбивается на предложения.  
2. Для каждого предложения вычисляется быстрый локальный вектор.  
3. Вычисляется косинусное расстояние между соседними предложениями.  
4. Если расстояние превышает определенный порог (threshold), система понимает, что произошла смена темы, и ставит границу чанка.

Это позволяет Django-приложению отправлять во внешний API (OpenAI) фрагменты, которые являются тематически завершенными. Например, вступление к статье не будет склеено с первым техническим разделом только потому, что они "влезают" в 500 токенов.

#### **Концепция Late Chunking (Поздний чанкинг)**

chonkie также поддерживает экспериментальный метод "Late Chunking". В традиционном RAG контекст теряется при нарезке. В Late Chunking сначала вычисляется эмбеддинг всего документа (или большого окна), и только потом из "карты" векторов вырезаются чанки. Это позволяет каждому маленькому чанку нести в себе "отпечаток" глобального контекста документа. Хотя этот метод требует более сложной реализации на стороне хранения (нужна поддержка со стороны векторного хранилища или специфическая структура данных), chonkie предоставляет инструменты для его подготовки на уровне Python.

### **2.3. AI-Chunking: Структурная осведомленность**

Библиотека **ai-chunking** занимает нишу обработки сложно-структурированных документов. В то время как другие инструменты работают с текстом как с потоком, ai-chunking пытается понять иерархию документа.  
Её ключевой компонент — **SectionBasedSemanticChunker**. Этот алгоритм идеально подходит для документации, юридических текстов или научных статей, написанных в формате Markdown или имеющих четкую структуру заголовков. Он гарантирует, что границы чанка никогда не пройдут посередине важной секции, если это возможно. Более того, библиотека позволяет генерировать богатые метаданные для каждого чанка (например, "этот чанк относится к разделу 3.1, который вложен в Главу 3"), что критически важно для фильтрации результатов поиска в Postgres.

### **2.4. Сравнительный анализ инструментов локального чанкинга**

Ниже приведена таблица, помогающая выбрать инструмент исходя из архитектурных требований проекта на Django.

| Характеристика | Semchunk | Chonkie | AI-Chunking |
| :---- | :---- | :---- | :---- |
| **Основной принцип** | Рекурсивный, Token-aware | Семантический (Embedding-based) | Структурный (Header/Section aware) |
| **Скорость** | Экстремально высокая (без ML) | Высокая (зависит от локальной модели) | Средняя (анализ структуры) |
| **Зависимости** | tiktoken (минимальные) | model2vec / sentence-transformers | tiktoken, опционально LLM API |
| **Точность границ** | Синтаксическая (абзацы, предложения) | Семантическая (смена темы) | Иерархическая (структура документа) |
| **Использование RAM** | Минимальное (\<100MB) | Среднее (загрузка мини-модели 50-200MB) | Низкое |
| **Рекомендация** | Для массовой обработки, логов, чатов | Для RAG по статьям, блогам, знаниям | Для технической документации, договоров |

Для большинства задач общего назначения в стеке Django \+ Postgres в 2025 году **Chonkie** с использованием SemanticChunker (с быстрой локальной моделью) представляется оптимальным балансом между качеством поиска и сложностью внедрения. Если же ресурсы сервера ограничены, **Semchunk** остается непревзойденным по эффективности решением.

## **3\. PostgreSQL как векторная база данных: Архитектура pgvector 2025**

Выбор PostgreSQL с расширением pgvector в качестве основы для хранения векторов является стратегическим решением, позволяющим консолидировать данные. К декабрю 2025 года pgvector перестал быть просто "плагином" и превратился в зрелую платформу для векторного поиска, поддерживающую квантование, разреженные векторы и продвинутые индексы.

### **3.1. Типы данных и оптимизация хранения**

В версии pgvector 0.8.0+ доступны несколько типов данных, выбор которых существенно влияет на производительность и стоимость инфраструктуры.

#### **Тип vector (FP32)**

Классический тип данных, хранящий каждое измерение как 32-битное число с плавающей точкой. Обеспечивает наивысшую точность (recall), но потребляет много памяти. Для вектора размерности 1536 (OpenAI text-embedding-3-small) одна запись занимает около 6 КБ. При миллионе записей индекс и таблица займут около 6-10 ГБ RAM, что может быть критично для стандартных инстансов Postgres.

#### **Тип halfvec (FP16)**

Нововведение 2025 года, ставшее стандартом де\-факто для большинства RAG-систем. halfvec использует 16-битные числа с плавающей точкой. Тесты показывают, что потеря точности при поиске составляет доли процента, при этом потребление памяти сокращается ровно в два раза. **Архитектурная рекомендация:** Для Django-проекта используйте halfvec для хранения эмбеддингов OpenAI. Это позволит хранить в два раза больше векторов на том же оборудовании без заметного ухудшения качества поиска.

#### **Тип bit (Binary Quantization)**

Для экстремально больших наборов данных (десятки миллионов векторов) pgvector поддерживает бинарное квантование. Вектор сжимается до набора битов. Хотя это значительно ускоряет поиск (используется расстояние Хэмминга), это требует поддержки со стороны модели эмбеддингов (модель должна быть обучена или адаптирована для бинарного представления) и может существенно снизить точность для нюансированных запросов.

### **3.2. Стратегии индексации: HNSW против IVFFlat**

Выбор индекса — это компромисс между скоростью поиска, скоростью вставки и потреблением памяти.

#### **HNSW (Hierarchical Navigable Small World)**

Это "золотой стандарт" векторного поиска в 2025 году. HNSW строит многослойный граф, позволяя находить ближайших соседей за логарифмическое время.

* **Преимущества:** Высочайшая скорость чтения (Queries Per Second), отличный recall, отсутствие необходимости в фазе "обучения" индекса.  
* **Недостатки:** Высокое потребление памяти (граф нужно держать в RAM для скорости), медленная вставка данных (построение графа затратно).  
* **Тюнинг:** Параметры m (количество связей на узел) и ef\_construction (размер списка кандидатов при построении) позволяют балансировать между скоростью и точностью. Для text-embedding-3-small (1536 dim) рекомендуемые стартовые значения: m=16, ef\_construction=64.

#### **IVFFlat (Inverted File Flat)**

Индекс, основанный на кластеризации. Векторное пространство делится на списки (кластеры), и поиск идет только в ближайших списках.

* **Преимущества:** Компактность, быстрое построение.  
* **Недостатки:** Требует предварительного обучения (обучается на самих данных), чувствителен к изменению распределения данных.  
* **Вердикт:** В 2025 году для RAG-систем IVFFlat используется редко, уступая место HNSW, кроме случаев с очень жесткими ограничениями по памяти.

### **3.3. Масштабируемость: pgvectorscale и DiskANN**

Для проектов, перерастающих рамки оперативной памяти одного сервера, экосистема Postgres в 2025 году предлагает расширение **pgvectorscale** (разработка Timescale). Оно реализует алгоритм **StreamingDiskANN**, который позволяет хранить индекс на быстром NVMe SSD, а не в RAM. Это меняет экономику проекта: вместо дорогих серверов с терабайтами RAM можно использовать инстансы с быстрым диском. Хотя для старта проекта на Django это может быть избыточным (premature optimization), знание этой возможности позволяет уверенно выбирать Postgres, зная, что есть путь для вертикального масштабирования.

## **4\. Паттерны интеграции в Django ORM**

Интеграция векторного поиска в Django требует пересмотра некоторых привычных паттернов работы с ORM. Стандартный подход "одна модель — одна сущность" усложняется необходимостью хранить чанки.

### **4.1. Проектирование схемы данных**

Рекомендуется использовать схему "Документ — Чанк" (One-to-Many). Документ хранит глобальные метаданные (автор, дата, URL источника), а Чанк — вектор и фрагмент текста.  
`from django.db import models`  
`from pgvector.django import VectorField, HnswIndex`

`class SourceDocument(models.Model):`  
    `title = models.CharField(max_length=255)`  
    `created_at = models.DateTimeField(auto_now_add=True)`  
    `metadata = models.JSONField(default=dict, help_text="Глобальные метаданные документа")`

`class TextChunk(models.Model):`  
    `document = models.ForeignKey(SourceDocument, on_delete=models.CASCADE, related_name='chunks')`  
    `content = models.TextField()`  
    `chunk_index = models.PositiveIntegerField()`  
      
    `# Используем vector(1536) для OpenAI text-embedding-3-small`  
    `# В Django 5.x+ и pgvector-python можно использовать HalfVectorField, если БД поддерживает`  
    `embedding = VectorField(dimensions=1536, help_text="Векторное представление чанка")`

    `class Meta:`  
        `indexes =,`  
                `m=16,`  
                `ef_construction=64,`  
                `opclasses=['vector_cosine_ops']`  
            `),`  
            `# Индекс для быстрого поиска по документу`  
            `models.Index(fields=['document', 'chunk_index']),`  
        `]`

### **4.2. Менеджеры и QuerySet для векторизации**

Вместо того чтобы разбрасывать логику вызова API OpenAI по контроллерам (Views), следует инкапсулировать её в кастомный менеджер или сервисный слой. Это соответствует принципу "Thick Models, Thin Views".  
**Проблема N+1:** При поиске чанков часто нужно отображать информацию о родительском документе. Обязательно используйте select\_related('document') в поисковых запросах.  
**Пример реализации менеджера с локальным чанкингом:**  
`import tiktoken`  
`from semchunk import chunkerify`  
`from django.conf import settings`  
`from openai import OpenAI`

`class TextChunkManager(models.Manager):`  
    `def ingest_document(self, document_instance, full_text):`  
        `"""`  
        `Полный цикл: Чанкинг -> Векторизация -> Сохранение`  
        `"""`  
        `# 1. Локальный чанкинг (используем semchunk для скорости)`  
        `encoder = tiktoken.get_encoding("cl100k_base") # Энкодер для моделей OpenAI`  
        `# Устанавливаем chunk_size чуть меньше лимита, чтобы оставить место для промпта`  
        `chunker = chunkerify(encoder, chunk_size=512)`   
        `text_chunks = chunker(full_text)`  
          
        `if not text_chunks:`  
            `return`

        `# 2. Векторизация через API (Batch request)`  
        `# Отправляем батчами, чтобы не упереться в лимиты API по размеру тела запроса`  
        `client = OpenAI(api_key=settings.OPENAI_API_KEY)`  
        `embeddings =`  
          
        `# Простая реализация батчинга по 20 чанков`  
        `BATCH_SIZE = 20`  
        `for i in range(0, len(text_chunks), BATCH_SIZE):`  
            `batch = text_chunks`  
            `response = client.embeddings.create(`  
                `input=batch,`  
                `model="text-embedding-3-small"`  
            `)`  
            `embeddings.extend([data.embedding for data in response.data])`

        `# 3. Сохранение (Bulk Create для производительности)`  
        `chunk_objects = [`  
            `self.model(`  
                `document=document_instance,`  
                `content=text,`  
                `embedding=vector,`  
                `chunk_index=idx`  
            `)`  
            `for idx, (text, vector) in enumerate(zip(text_chunks, embeddings))`  
        `]`  
          
        `self.bulk_create(chunk_objects)`

Такой подход обеспечивает атомарность (в рамках транзакции Django) и эффективность работы с сетью и базой данных.

## **5\. Гибридный поиск: Теория и реализация (Reciprocal Rank Fusion)**

Один из главных недостатков чисто векторного поиска — его неспособность находить точные совпадения, такие как артикулы, специфические аббревиатуры или редкие имена собственные. Семантически "PostgreSQL" и "Database" близки, но если пользователь ищет "Ошибка 404 в модуле X", векторный поиск может вернуть статьи про ошибки вообще, а лексический поиск (Full-Text Search) найдет точное вхождение.  
Гибридный поиск объединяет эти два подхода. Однако возникает проблема масштаба: векторное расстояние (косинусное) обычно находится в диапазоне (или в зависимости от реализации), а релевантность полнотекстового поиска (BM25 или ts\_rank) не ограничена сверху и зависит от частоты слов. Просто сложить 0.85 \+ 15.4 нельзя — лексический скор "задавит" семантический.

### **5.1. Алгоритм Reciprocal Rank Fusion (RRF)**

Решением является алгоритм **RRF**. Он игнорирует абсолютные значения скоров и работает только с **рангами** (позициями в выдаче).  
Формула RRF для документа d:  
Где:

* rank\_i(d) — позиция документа d в выдаче поисковой системы i (начиная с 1).  
* k — константа сглаживания. Эмпирически установлено, что k=60 дает наилучшие результаты.

Смысл константы k: она не дает документам, занявшим первые места в одной системе, но отсутствующим в другой, полностью доминировать в итоговой выдаче. Это "смягчает" влияние выбросов.

### **5.2. Реализация RRF в Django (Raw SQL)**

Хотя можно выполнить два запроса в Python и слить их словарями, наиболее производительный способ — выполнить это одним SQL-запросом, используя Common Table Expressions (CTE). Это позволяет базе данных самой выполнить сортировку и лимитирование, минимизируя трафик данных между БД и приложением.  
Ниже приведен пример реализации метода поиска в кастомном менеджере.  
`from django.db import connection`

`def hybrid_search(query_text, query_vector, k=60, limit=20):`  
    `sql = """`  
    `WITH semantic_search AS (`  
        `SELECT id,`   
               `-- Вычисляем ранг на основе векторного расстояния`  
               `RANK() OVER (ORDER BY embedding <=> %s) as rank_vec`  
        `FROM myapp_textchunk`  
        `ORDER BY embedding <=> %s`  
        `LIMIT 100 -- Берем топ-100 кандидатов от векторов`  
    `),`  
    `keyword_search AS (`  
        `SELECT id,`   
               `-- Вычисляем ранг на основе FTS (ts_rank_cd)`  
               `RANK() OVER (ORDER BY ts_rank_cd(to_tsvector('english', content), plainto_tsquery('english', %s)) DESC) as rank_fts`  
        `FROM myapp_textchunk`  
        `WHERE to_tsvector('english', content) @@ plainto_tsquery('english', %s)`  
        `LIMIT 100 -- Берем топ-100 кандидатов от слов`  
    `)`  
    `SELECT t.id, t.content, t.chunk_index,`  
           `-- Формула RRF`  
           `COALESCE(1.0 / (%s + s.rank_vec), 0.0) +`   
           `COALESCE(1.0 / (%s + k.rank_fts), 0.0) as rrf_score`  
    `FROM myapp_textchunk t`  
    `-- Full Outer Join не поддерживается эффективно для таких выборок,`  
    `-- поэтому делаем LEFT JOIN к основной таблице от объединенных ID`  
    `RIGHT JOIN (`  
        `SELECT id FROM semantic_search`  
        `UNION`  
        `SELECT id FROM keyword_search`  
    `) as merged_ids ON t.id = merged_ids.id`  
    `LEFT JOIN semantic_search s ON t.id = s.id`  
    `LEFT JOIN keyword_search k ON t.id = k.id`  
    `ORDER BY rrf_score DESC`  
    `LIMIT %s;`  
    `"""`  
      
    `# Параметры запроса дублируются, так как плейсхолдеры позиционные`  
    `params =`  
      
    `with connection.cursor() as cursor:`  
        `cursor.execute(sql, params)`  
        `columns = [col for col in cursor.description]`  
        `results = [dict(zip(columns, row)) for row in cursor.fetchall()]`  
          
    `return results`

**Анализ реализации:**

1. **plainto\_tsquery('english',...)**: Эта функция Postgres преобразует пользовательский ввод в безопасный запрос для FTS, удаляя стоп-слова и знаки препинания. Важно указать 'english', так как запрос пользователя на английском.  
2. **\<=\>**: Оператор косинусного расстояния в pgvector.  
3. **Производительность**: Использование LIMIT 100 внутри CTE критически важно. Мы не ранжируем всю таблицу, а берем только лучших кандидатов от каждой стратегии для слияния.

## **6\. Операционные аспекты и оптимизация**

Построение системы — это половина дела. Вторая половина — её эксплуатация.

### **6.1. Обслуживание индексов (Vacuuming)**

Индексы HNSW в Postgres требуют периодического обслуживания. При активном обновлении или удалении чанков в индексе накапливается "мусор", что снижает recall. Важно настроить процесс VACUUM для таблиц с векторами, хотя Postgres делает это автоматически (autovacuum), для высоконагруженных систем может потребоваться ручной тюнинг параметров autovacuum\_vacuum\_scale\_factor.

### **6.2. Кэширование запросов**

Генерация эмбеддинга запроса через API OpenAI занимает время (обычно 100-300 мс). Если пользователи часто задают одинаковые или похожие вопросы, имеет смысл кэшировать вектор запроса в Redis. Ключом кэша может быть хеш от нормализованного текста запроса. Это снижает латентность и затраты на API.

### **6.3. Проблема "холодного старта" индекса**

При создании индекса HNSW (ef\_construction) потребляется много CPU. Не делайте создание индекса частью обычной миграции, если в таблице уже есть миллионы записей — это заблокирует деплой надолго. Используйте Concurrently создание индексов или запускайте их как отдельные задачи обслуживания.

### **6.4. Мониторинг качества (RAG Evaluation)**

Как понять, что ваш чанкинг или RRF работают хорошо? В 2025 году стандартом стала библиотека **RAGAS** или аналогичные фреймворки оценки. Создайте тестовый набор вопросов и "золотых" ответов. Периодически прогоняйте систему через этот набор, измеряя метрики Hit Rate (попал ли правильный чанк в топ-5) и MRR (Mean Reciprocal Rank). Без метрик оптимизация параметров k в RRF или chunk\_size превращается в гадание.

## **7\. Заключение**

К концу 2025 года создание собственной системы семантического поиска на стеке **Django \+ Postgres** стало не просто возможным, но и предпочтительным архитектурным паттерном для большинства приложений.

1. **Локальный чанкинг** перестал быть узким местом благодаря инструментам вроде **Semchunk** (для скорости) и **Chonkie** (для семантической точности), позволяя готовить данные "дома" перед отправкой в облака.  
2. **Pgvector** с типами halfvec и индексами HNSW обеспечивает производительность, сопоставимую со специализированными базами данных, при нулевых затратах на синхронизацию данных.  
3. **Гибридный поиск** через **RRF**, реализованный средствами SQL внутри Django, решает фундаментальную проблему точности векторного поиска, объединяя лучшее из миров семантики и лексики.

Эта архитектура является надежным, масштабируемым и экономически эффективным фундаментом для любых интеллектуальных поисковых систем и RAG-приложений.

#### **Источники**

1\. PostgreSQL as a Vector Database: A Pgvector Tutorial \- Tiger Data, https://www.tigerdata.com/learn/postgresql-as-a-vector-database-a-pgvector-tutorial 2\. pgvector/pgvector: Open-source vector similarity search for Postgres \- GitHub, https://github.com/pgvector/pgvector 3\. Chunk Better With Chonkie: How Late Chunking Improves Text Segmentation \- Towards AI, https://pub.towardsai.net/easy-late-chunking-with-chonkie-7f05e5916997 4\. A Visual Exploration of Semantic Text Chunking \- Towards Data Science, https://towardsdatascience.com/a-visual-exploration-of-semantic-text-chunking-6bb46f728e30/ 5\. semchunk \- PyPI, https://pypi.org/project/semchunk/ 6\. isaacus-dev/semchunk: A fast, lightweight and easy-to-use ... \- GitHub, https://github.com/isaacus-dev/semchunk 7\. Using Chonkie \- DEV Community, https://dev.to/aairom/using-chonkie-4glg 8\. Open Source \- Chonkie Documentation, https://docs.chonkie.ai/common/open-source 9\. Semantic Chunker \- Chonkie Documentation, https://docs.chonkie.ai/oss/chunkers/semantic-chunker 10\. ai-chunking \- PyPI, https://pypi.org/project/ai-chunking/ 11\. pgvector support for Python \- GitHub, https://github.com/pgvector/pgvector-python 12\. Run a hybrid vector similarity search | AlloyDB for PostgreSQL, https://docs.cloud.google.com/alloydb/docs/ai/run-hybrid-vector-similarity-search 13\. RAG Is More Than Just Vector Search \- Tiger Data, https://www.tigerdata.com/learn/rag-is-more-than-just-vector-search 14\. Hybrid Search Using Reciprocal Rank Fusion in SQL \- SingleStore, https://www.singlestore.com/blog/hybrid-search-using-reciprocal-rank-fusion-in-sql/ 15\. Understanding Reciprocal Rank Fusion (RRF) in Retrieval-Augmented Systems, https://dev.to/master-rj/understanding-reciprocal-rank-fusion-rrf-in-retrieval-augmented-systems-52kc 16\. Hybrid search | Supabase Docs, https://supabase.com/docs/guides/ai/hybrid-search 17\. Performing raw SQL queries | Django documentation, https://docs.djangoproject.com/en/6.0/topics/db/sql/ 18\. RAG boilerplate with semantic/propositional chunking, hybrid search (BM25 \+ dense), LLM reranking, query enhancement agents, CrewAI orchestration, Qdrant vector search, Redis/Mongo sessioning, Celery ingestion pipeline, Gradio UI, and an evaluation suite (Hit-Rate, MRR, hybrid configs). \- GitHub, https://github.com/mburaksayici/RAG-Boilerplate
# Фаза 4.8: Пайплайн индексации

## Цель

Объединить сервисы в единый пайплайн индексации постов.

---

## Контекст

Предыдущие фазы создали отдельные компоненты:
- 4.5: `extract_sections()` — парсинг структуры
- 4.6: `chunk_content()` — разбиение на чанки
- 4.7: `vectorize_chunks()` — получение эмбеддингов

Теперь нужно объединить их в единый пайплайн, который:
1. Принимает пост
2. Извлекает структуру из Markdown
3. Нарезает на чанки с маршрутизацией
4. Векторизует новые чанки
5. Сохраняет в БД (bulk_create для производительности)

Пайплайн должен быть идемпотентным: повторный вызов не создаёт дубликатов и не тратит API-вызовы на неизменённый контент.

**Философия:** End-to-end автоматизация от Markdown до векторов.

**Документация:**
- [ИИ поиск на базе смыслов — концепция](../researches/report_6/ИИ%20поиск%20на%20базе%20смыслов%20-%20концепция.md) — "ЭТАП 2: Движок индексации"
- [Технический отчёт: Chonkie, PostgreSQL, pgvector](../researches/report_6/Технический%20отчет_%20Chonkie,%20Postgresql,%20pgvector.md) — раздел 2.3 "Архитектура пайплайна ингестии"

---

## Задачи

### Создание главного сервиса

- [ ] Создать файл `blog/services/indexing_pipeline.py`
- [ ] Определить функцию `index_post(post: Post) -> IndexingResult`
- [ ] Создать dataclass `IndexingResult` с полями: `chunks_created`, `chunks_skipped`, `tokens_used`, `cost_usd`

### Логика пайплайна

- [ ] Получить Markdown-контент из `post.content`
- [ ] Вызвать `extract_sections(content)` для получения структуры
- [ ] Вызвать `chunk_content(sections)` для нарезки
- [ ] Вычислить `content_hash` для каждого чанка
- [ ] Найти существующие чанки поста с совпадающими хешами
- [ ] Отфильтровать чанки, которые нужно векторизовать
- [ ] Вызвать `vectorize_chunks(new_chunks)`
- [ ] Удалить старые чанки поста, которых нет в новом списке
- [ ] Сохранить новые чанки через `TextChunk.objects.bulk_create()`

### Оптимизация сохранения

- [ ] Использовать `bulk_create` с `update_conflicts=True` для upsert-логики
- [ ] Или: удалить все старые чанки перед созданием новых (проще, но менее эффективно)
- [ ] Выбрать стратегию в зависимости от частоты обновлений

### Management-команда

- [ ] Создать `blog/management/commands/index_posts.py`
- [ ] Команда `python manage.py index_posts` индексирует все посты
- [ ] Опции: `--post-id` для индексации одного поста, `--force` для принудительной переиндексации
- [ ] Вывод прогресса: "Indexed post {id}: {chunks_created} chunks, ${cost} spent"

### Интеграция с сигналами

- [ ] Обновить обработчик `post_save` из фазы 4.4
- [ ] Вызывать `index_post(instance)` при сохранении поста
- [ ] Добавить флаг `skip_indexing` для случаев, когда индексация не нужна

### Генерация search_vector

- [ ] После сохранения чанков обновить поле `search_vector` для FTS
- [ ] Использовать `to_tsvector('russian', content)` для русского текста
- [ ] Можно реализовать через триггер PostgreSQL или в коде Django

---

## Тестирование

- [ ] Создать пост с Markdown-контентом
- [ ] Запустить `python manage.py index_posts --post-id=1`
- [ ] Проверить, что чанки созданы в БД с эмбеддингами
- [ ] Повторно запустить команду — новые чанки не должны создаваться
- [ ] Изменить контент поста и запустить снова — должны обновиться

---

## Коммит

```
phase 4.8 feat: Пайплайн индексации
- Создан blog/services/indexing_pipeline.py
- Объединены extract_sections, chunk_content, vectorize_chunks
- Добавлена management-команда index_posts
- Интегрировано с сигналом post_save
- Реализовано обновление search_vector для FTS
```

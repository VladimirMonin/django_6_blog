# Фаза 4.6: Сервис чанкинга (Chonkie)

## Цель

Интегрировать библиотеку Chonkie для интеллектуального разбиения контента на чанки.

---

## Контекст

После извлечения структуры (фаза 4.5) каждый блок контента нужно разбить на чанки оптимального размера для векторизации. Размер чанка — компромисс:
- Слишком большой → потеря точности поиска
- Слишком маленький → потеря контекста

**Chonkie** (v1.0.4+) — легковесная библиотека (~15-21 МБ), специализирующаяся на чанкинге для RAG. Ключевые преимущества:
- 33x ускорение по сравнению с LangChain
- `SemanticChunker` — границы по смене темы (косинусное расстояние между предложениями)
- `CodeChunker` — AST-парсинг через tree-sitter, сохранение целостности функций/классов
- `RecursiveChunker` — сохранение структуры Markdown

**Маршрутизация контента:**
- Блоки CODE → `CodeChunker` (AST)
- Блоки TEXT → `SemanticChunker` или `RecursiveChunker`
- Таблицы → специальная обработка (строки с заголовками)

**Философия:** Код нельзя резать посередине функции — AST это гарантирует.

**Документация:**
- [Технический отчёт: Chonkie, PostgreSQL, pgvector](../researches/report_6/Технический%20отчет_%20Chonkie,%20Postgresql,%20pgvector.md) — раздел 2 "Слой ингестии: Chonkie"
- [Django, pgvector: семантический поиск локально](../researches/report_6/Django,%20pgvector_%20семантический%20поиск%20локально.md) — раздел 2 "Эволюция инструментов локального чанкинга"

---

## Задачи

### Установка Chonkie

- [ ] Установить через Poetry: `poetry add chonkie[semantic]`
- [ ] Установить tiktoken для подсчёта токенов: `poetry add tiktoken`
- [ ] Проверить импорт: `from chonkie import SemanticChunker, RecursiveChunker`

### Создание сервиса

- [ ] Создать файл `blog/services/chunking_service.py`
- [ ] Определить функцию `chunk_content(sections: list[Section]) -> list[ChunkData]`
- [ ] Создать dataclass `ChunkData` с полями: `content`, `chunk_type`, `section_title`, `position_index`, `code_language`

### Настройка чанкеров

- [ ] Инициализировать `SemanticChunker` с параметрами:
  - `chunk_size=512` (токенов)
  - `similarity_threshold` для определения границ
- [ ] Инициализировать `CodeChunker` для Python, JavaScript, и других языков
- [ ] Настроить fallback: если AST-парсинг не удался — использовать текстовый чанкинг

### Логика маршрутизации

- [ ] Для каждого `ContentBlock` из секции определить чанкер по `block_type`
- [ ] CODE → `CodeChunker` с указанием языка
- [ ] TEXT → `SemanticChunker`
- [ ] MERMAID → пока пропускаем (Phase 5)
- [ ] Собрать результаты с сохранением `position_index`

### Обработка ошибок

- [ ] Обернуть AST-парсинг в try/except
- [ ] При ошибке парсинга кода — логировать и использовать текстовый fallback
- [ ] Сохранять информацию о проблемных блоках для отладки

### Оптимизация размера чанков

- [ ] Использовать tiktoken для точного подсчёта токенов
- [ ] Целевой размер: 400-512 токенов (оставляем место для промпта)
- [ ] Настроить overlap (перекрытие) ~50 токенов для сохранения контекста на границах

---

## Тестирование

- [ ] Подготовить тестовую секцию с Python-кодом
- [ ] Проверить, что функции не разрезаются посередине
- [ ] Подготовить длинный текстовый блок
- [ ] Проверить, что семантический чанкер разбивает по смене темы

---

## Коммит

```
phase 4.6 feat: Сервис чанкинга (Chonkie)
- Создан blog/services/chunking_service.py
- Интегрированы SemanticChunker и CodeChunker
- Реализована маршрутизация CODE/TEXT
- Добавлен fallback при ошибках AST-парсинга
```
